{"cells":[{"cell_type":"code","source":["!python -m pip install --upgrade pip"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c50141d9-c12d-47ea-ac99-13b3dbda64ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["!pip install -r requirements38.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"27e0852f-12c8-4fb1-9c2d-06bc6c3635d8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["!pip install azure-storage-file-datalake\n!pip install adlfs\n!pip install fsspec"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d477b1e-6d40-484a-9f3e-04afee82b566","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["!pip install trieregex"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"985732d3-9340-4b05-a3b3-aff93a1d7131","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import multiprocessing as mp\nimport sys\nfrom datetime import datetime\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom ast import literal_eval\nimport string"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e56f498-a71b-4935-ad39-d9b758567f78","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(f'Started at {datetime.now()}')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37b6365a-457f-4019-b4b6-38d11ae04274","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Connect to Azure Data Storage"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46e8cad7-b717-4593-8c6c-f6faaf08ec17","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import os, uuid, sys\nfrom azure.storage.filedatalake import DataLakeServiceClient\nfrom azure.core._match_conditions import MatchConditions\nfrom azure.storage.filedatalake._models import ContentSettings\nfrom azure.storage.blob import BlobServiceClient"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"828c6695-7764-4358-b09e-9f0b0faf5191","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def initialize_storage_account(storage_account_name, storage_account_key):\n    \n    try:  \n        global service_client\n\n        service_client = DataLakeServiceClient(account_url=\"{}://{}.dfs.core.windows.net\".format(\n            \"https\", storage_account_name), credential=storage_account_key)\n    \n    except Exception as e:\n        print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3bc4bfce-2e75-464d-b250-0f46632228a5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def list_directory_contents(container_name,my_dir):\n    file_list = []\n    try:\n        \n        file_system_client = service_client.get_file_system_client(file_system=container_name)\n\n        paths = file_system_client.get_paths(path=my_dir)\n\n        for path in paths:\n            file_list.append(path.name)\n\n    except Exception as e:\n     print(e)\n    \n    return file_list"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39c6678d-75b3-4686-a247-ce3b0110b13c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["storage_account = \"legoaistorage\"\nstorage_account_key = \"vOHAjE9vOHaxqmTRxIYETQbYlPvvFpJQ7xfky8tuWBRE9E6IbfM87ERkGcqqiHfMHs+WnEt907r6+AStjIYXlA==\"\ninitialize_storage_account(storage_account,storage_account_key)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fceeeb86-9d2e-4955-be89-6db9a184e273","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["### Spark Configuration\nspark.conf.set(\"fs.azure.account.key.\"+ storage_account +\".dfs.core.windows.net\", storage_account_key)\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a2bb4ff-3609-4f32-9305-3debb4144d80","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["prefix_path = \"abfss://datascience-dataset@legoaistorage.dfs.core.windows.net/\"\nconnect_str = 'DefaultEndpointsProtocol=https;AccountName={};AccountKey={}'.format(storage_account,storage_account_key)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e324870f-c139-4eb5-851f-6992a75f6680","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["### Getting the data and other required information from each source\ncontainer_name = 'datascience-dataset'\ndata_content = list_directory_contents(container_name,'Source_Data')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"29dc508d-c63a-4c39-910c-184f501a9966","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["### Subset data which is of json file format\njson_data_content = [content for content in data_content if content.endswith('.json')]\n\njson_meta = []\nfor cont in json_data_content:\n    json_meta.append([cont.split('/')[1],cont.split('/')[-3],cont.split('/')[-2],cont.split('/')[-1],cont])\n    \njson_meta_df =  pd.DataFrame(json_meta,columns= ['source','location','reponame','filename','filepath'])\njson_meta_df = json_meta_df.reset_index(drop=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"859f5344-df36-4b28-96b6-291afe414261","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def data_conversion(filename,filepath):\n    \n    ## Identify the file extension\n    file_extension = filename.rsplit('.',1)[1]\n    full_filepath = \"abfss://datascience-dataset@legoaistorage.dfs.core.windows.net/\"+filepath\n    \n    ## Read the data based on extension\n    if file_extension == 'json':\n        data_dict = spark.read.option(\"multiline\", \"true\").json(full_filepath)\n        data = data_dict.toPandas()\n\n    elif file_extension == 'parquet':\n        data = spark.read.parquet(full_filepath)\n            \n    elif file_extension == 'csv':\n        data = spark.read.format(\"csv\").option(\"mode\", \"PERMISSIVE\").load(full_filepath)\n        \n    elif file_extension == 'txt':\n        data = spark.read.text(full_filepath)\n    \n    else:\n        data = pd.DataFrame(columns=['id','table_name','column_name','values']) \n    \n    return data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ed54b759-30eb-4be8-9df6-e21cf3ad4702","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def meta_information_check(data_df,filename,reponame):\n    \n    ### ID creation for each df\n    if 'id' in data_df.columns:\n        \n        if data_df['id'].nunique() != data_df.shape[0]:\n            data_df['id'] = [i for i in range(len(data_df))]        \n            \n    elif 'column_id' in data_df.columns:\n        data_df['id'] = data_df['column_id']\n    else:\n        data_df = data_df.reset_index(drop=True).reset_index()\n        data_df = data_df.rename(columns={'index':'id'})\n    \n    #### Mandatory column checks\n    if 'column_name' not in data_df.columns:\n        \n        if 'type' in data_df.columns:\n            data_df['column_name'] = data_df['type']\n        else:\n            data_df['column_name'] = ''   ### Need to check with             \n    \n    if 'table_name' not in data_df.columns:\n        data_df['table_name'] = filename.rsplit('.',1)[0]\n    \n    data_df = data_df.rename(columns={'column_values':'values','value':'values'})\n    \n    data_df['values'] = data_df.apply(lambda x: list(x['values']),axis=1)\n    print('Unique Type of values:', list(set([type(val) for val in data_df['values'].tolist()])))\n    \n    assert list(set([type(val) for val in data_df['values'].tolist()]))[0] == list\n    \n    if reponame == 'swastik':\n        data_df['repo_name'] = data_df['dataset_name']\n    else:\n        data_df['repo_name'] = reponame\n        \n    data_df['master_id'] = data_df.apply(lambda x: x['repo_name']+'$$##$$'+x['table_name']+'$$##$$'+x['column_name'],axis=1)    \n    assert data_df['master_id'].nunique()==data_df.shape[0]\n\n    return data_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf13b733-7adc-4ae8-ac38-2260d72030cf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for row in range(1): #len(json_meta_df)): \n    \n    status = 0\n    try:\n        \n        print('Feature Creation Started!!')\n        ## Required variables\n        reponame,filename,filepath = json_meta_df[['reponame','filename','filepath']].iloc[row].tolist()\n\n        ### Extracting data from json    \n        json_df = data_conversion(filename,filepath)\n\n        if json_df.shape[0] ==0:\n            continue\n        \n        ### Extracting meta data    \n        meta_json_df = meta_information_check(json_df,filename,reponame)\n        print('Meta Data Row Count: ',meta_json_df.shape)\n        \n    except Exception as e:\n        print(traceback.format_exc())\n        print(e)\n\n    print('Feature Creation Completed!!')\n    print(row,status)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b585bb7e-f68b-4d14-a251-e477e37bd15d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def special_token_repl(text: str, suffix: str):\n    replaced_text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n    replaced_text = re.sub(string=replaced_text,pattern=' +',repl=' ')\n    \n    if replaced_text == '':\n        replaced_text = 'unknown' + suffix\n    \n    return replaced_text"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"528125a0-d38e-4bed-b7c2-fbcbdd6bd959","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def normalise_whitespace(data):\n    if isinstance(data, str):\n        return re.sub(r\"\\s{2,}\", \" \", data.strip())\n    else:\n        return data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9591d09-ba45-4f49-af07-0cc775df1356","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ignoreList = ['#na','#n/a','na','n/a','none','nan','blank','blanks','nil','n.a.','n.a',\n             '\"#na\"','\"#n/a\"','\"na\"','\"n/a\"','\"none\"','\"nan\"','\"blank\"','\"blanks\"','\"nil\"','\"n.a.\"','\"n.a\"',\n             \"'#na'\",\"'#n/a'\",\"'na'\",\"'n/a'\",\"'none'\",\"'nan'\",\"'blank'\",\"'blanks'\",\"'nil'\",\"'n.a.'\",\"'n.a'\"]\n\ndef additional_processing(value):\n    \n    #print('Additional Processing:',value)\n    if value is None or pd.isnull(value) or str(value).lower() in ignoreList:\n      return_val = ''\n    else:\n      value = str(value).replace('\\xa0',' ').strip()\n      return_val = removeASCII(value)\n\n    return return_val"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b82da45b-7f1c-4e0a-a7a7-225b70e5aa59","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def normalise_string_whitespace(col_values):\n\n    master_id = col_values[0]\n    id = col_values[1]\n    dataset_name = col_values[2]\n    table_name = col_values[3]\n    column_name = col_values[4]\n    \n    normalized_values = list(map(normalise_whitespace, col_values[5:]))\n    \n    ### Removing the table and column name from values ## Added to remove features list\n    normalized_values = [val for val in normalized_values if str(val).lower() not in [dataset_name.lower() ,table_name.lower(),column_name.lower()]]\n    \n    normalized_values_upd = [master_id] + [id] + [dataset_name] + [table_name] + [column_name] + normalized_values\n    return normalized_values_upd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2217d31f-9065-44e8-af7e-1bcf4659d0b0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#### Remove ASCII Characters from the data\ndef removeASCII(strs):\n    return ''.join([char for word in str(strs) for char in word if ord(char)<128])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1cdcb3a-3eb4-4bd0-9d6b-2c5121c2d83b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def cleaning_data(col_values):\n    \n    master_id = col_values[0]\n    id = col_values[1]\n    dataset_name = col_values[2]\n    table_name = col_values[3]\n    column_name = col_values[4]\n    col_values = col_values[5:]\n    \n    table_name_clean = special_token_repl(table_name,suffix='_table_name')\n    column_name_clean = special_token_repl(column_name,suffix='_column_name')\n    \n    cleaned_values = [additional_processing(val) for val in col_values]\n    return cleaned_values"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59ca0791-6b6b-4ac0-863f-10313296a9e9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def checkInt(strs):\n        \n    ### If integer type then return else return 0\n    if isinstance(strs,int):\n        return 1\n    elif isinstance(strs,float):\n        return 0\n    else:\n        try:\n            int(strs)\n            return 1\n        except:\n            return 0"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3e76417-b1c4-4cda-8caa-600d8a42b5e1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["### Check if the  data is of Float type or not\ndef checkFloat(strs):\n        \n    ### If Float type then return else return 0\n    if isinstance(strs,float):\n        return 1\n    else:\n        try:\n            if checkInt(strs):\n                return 0\n            strs = float(strs)\n            if strs != np.inf:\n                return 1\n            else:\n                return 0\n        except:\n            return 0"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"414e1c5b-caac-487b-9f51-c0edafca6587","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8e476025-ac15-4722-a08a-4b72a0313b8e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def featureCreation(values):\n    uniq_clean_values = list(set(values))\n    total_vals = len(values)\n    uniq_vals = len(uniq_clean_values)\n    \n    int_ratio = np.mean([checkInt(val) for val in values])\n    float_ratio = np.mean([checkFloat(val) for val in values])\n    \n    print(int_ratio,float_ratio)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d891209-e56a-48ed-bc3a-cf39163d6ea8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for eachRow in range(1):#meta_json_df.shape[0]):\n    clean_row_values = cleaning_data(meta_json_df['values'][0])\n    featureCreation(clean_row_values)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7b7fec2-8ffa-46bc-be5f-2d3f5ee80eea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c8b0b97-ae46-4abb-bcb8-f6e154bbcb19","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import re\nfrom trieregex import TrieRegEx as TRE\n\nwords = ['\\d{4}-[0-1][0-9]-[0-3][0-9]']\n\n# Initialize class instance\ntre = TRE()\n\n# Add word(s)\ntre = TRE(*words)  # word(s) can be added upon instance creation, or after\n\n# Create regex pattern from the trie\ntre.regex()  # Returns: '(?:tange(?:rine|lo)|grape(?:fruit)?|kumquat)'\nprint(tre.regex())\n\n# # Add boundary context and compile for matching\npattern = re.compile(f'\\\\b{tre.regex()}\\\\b')  # OR rf'\\b{tre.regex()}\\b'\nprint(pattern)\npattern.findall(\"check in January 2022-01-01\")  # Returns: ['kumquat']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7821925-fc80-4fc4-9b36-9530ad6afa56","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["re.findall(re.compile(words[0]),string='check in January 2022-01-01')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a10d907-edc4-44c7-87c9-36e529157155","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce81e977-3d8b-448d-85c5-d419db41de34","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Step-3.1: Rule based Approach","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1184679551609924}},"nbformat":4,"nbformat_minor":0}
